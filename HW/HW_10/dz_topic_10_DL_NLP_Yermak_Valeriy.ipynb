{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":3235802,"sourceType":"datasetVersion","datasetId":1961542}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-08-09T15:14:07.056309Z","iopub.execute_input":"2025-08-09T15:14:07.056731Z","iopub.status.idle":"2025-08-09T15:14:07.535858Z","shell.execute_reply.started":"2025-08-09T15:14:07.056709Z","shell.execute_reply":"2025-08-09T15:14:07.535237Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/email-spam-detection-dataset-classification/spam.csv\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# Imports and customizations\nimport os\nimport glob\nimport re\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport matplotlib.ticker as mtick\nimport nltk\ntry:\n    nltk.data.find('corpora/wordnet')\nexcept LookupError:\n    nltk.download('stopwords')\n\ntry:\n    nltk.data.find('corpora/wordnet')\nexcept LookupError:\n    nltk.download('wordnet')\n\ntry: \n    nltk.data.find('tokenizers/punkt')\nexcept LookupError:\n    nltk.download('punkt')\ntry:\n    nltk.data.find('corpora/omw-1.4')\nexcept LookupError:\n    nltk.download('omw-1.4')\nimport gensim.downloader as api\nimport xgboost as xgb\nimport string\n\n\nfrom pathlib import Path\nfrom tqdm.auto import tqdm\ntqdm.pandas()\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score, roc_auc_score, classification_report\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.model_selection import GridSearchCV\nfrom scipy.sparse import hstack","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-09T16:22:49.152254Z","iopub.execute_input":"2025-08-09T16:22:49.153027Z","iopub.status.idle":"2025-08-09T16:22:49.163717Z","shell.execute_reply.started":"2025-08-09T16:22:49.152974Z","shell.execute_reply":"2025-08-09T16:22:49.162982Z"}},"outputs":[{"name":"stderr","text":"[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n[nltk_data] Downloading package wordnet to /usr/share/nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\n[nltk_data] Downloading package omw-1.4 to /usr/share/nltk_data...\n[nltk_data]   Package omw-1.4 is already up-to-date!\n","output_type":"stream"}],"execution_count":47},{"cell_type":"code","source":"# Finding and downloading a dataset (working on Kaggle)\ninput_path = '/kaggle/input'\n\nif os.path.exists(input_path):\n    print(f\"–ü–∞–ø–∫–∞ {input_path} —ñ—Å–Ω—É—î.\")\n    for dirname, _, filenames in os.walk(input_path):\n        print(f\"üìÇ –ü–∞–ø–∫–∞: {dirname}\")\n        for filename in filenames:\n            print(f\"  - {filename}\")\nelse:\n    print(f\"–ü–∞–ø–∫–∞ {input_path} –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–∞. –ü–µ—Ä–µ–≤—ñ—Ä –ø—ñ–¥–∫–ª—é—á–µ–Ω–Ω—è –¥–∞—Ç–∞—Å–µ—Ç—É.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-09T15:14:47.158760Z","iopub.execute_input":"2025-08-09T15:14:47.159144Z","iopub.status.idle":"2025-08-09T15:14:47.164949Z","shell.execute_reply.started":"2025-08-09T15:14:47.159125Z","shell.execute_reply":"2025-08-09T15:14:47.164223Z"}},"outputs":[{"name":"stdout","text":"–ü–∞–ø–∫–∞ /kaggle/input —ñ—Å–Ω—É—î.\nüìÇ –ü–∞–ø–∫–∞: /kaggle/input\nüìÇ –ü–∞–ø–∫–∞: /kaggle/input/email-spam-detection-dataset-classification\n  - spam.csv\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"data_path = '/kaggle/input/email-spam-detection-dataset-classification/spam.csv'\ndf = pd.read_csv(data_path, encoding='latin-1')\n\nprint('Review first 5 rows of dataset:')\ndisplay(df.head(10))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-09T15:14:47.166380Z","iopub.execute_input":"2025-08-09T15:14:47.166577Z","iopub.status.idle":"2025-08-09T15:14:47.239284Z","shell.execute_reply.started":"2025-08-09T15:14:47.166561Z","shell.execute_reply":"2025-08-09T15:14:47.238732Z"}},"outputs":[{"name":"stdout","text":"Review first 5 rows of dataset:\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"     v1                                                 v2 Unnamed: 2  \\\n0   ham  Go until jurong point, crazy.. Available only ...        NaN   \n1   ham                      Ok lar... Joking wif u oni...        NaN   \n2  spam  Free entry in 2 a wkly comp to win FA Cup fina...        NaN   \n3   ham  U dun say so early hor... U c already then say...        NaN   \n4   ham  Nah I don't think he goes to usf, he lives aro...        NaN   \n5  spam  FreeMsg Hey there darling it's been 3 week's n...        NaN   \n6   ham  Even my brother is not like to speak with me. ...        NaN   \n7   ham  As per your request 'Melle Melle (Oru Minnamin...        NaN   \n8  spam  WINNER!! As a valued network customer you have...        NaN   \n9  spam  Had your mobile 11 months or more? U R entitle...        NaN   \n\n  Unnamed: 3 Unnamed: 4  \n0        NaN        NaN  \n1        NaN        NaN  \n2        NaN        NaN  \n3        NaN        NaN  \n4        NaN        NaN  \n5        NaN        NaN  \n6        NaN        NaN  \n7        NaN        NaN  \n8        NaN        NaN  \n9        NaN        NaN  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>v1</th>\n      <th>v2</th>\n      <th>Unnamed: 2</th>\n      <th>Unnamed: 3</th>\n      <th>Unnamed: 4</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>ham</td>\n      <td>Go until jurong point, crazy.. Available only ...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>ham</td>\n      <td>Ok lar... Joking wif u oni...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>spam</td>\n      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>ham</td>\n      <td>U dun say so early hor... U c already then say...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>ham</td>\n      <td>Nah I don't think he goes to usf, he lives aro...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>spam</td>\n      <td>FreeMsg Hey there darling it's been 3 week's n...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>ham</td>\n      <td>Even my brother is not like to speak with me. ...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>ham</td>\n      <td>As per your request 'Melle Melle (Oru Minnamin...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>spam</td>\n      <td>WINNER!! As a valued network customer you have...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>spam</td>\n      <td>Had your mobile 11 months or more? U R entitle...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":4},{"cell_type":"code","source":"print('\\n Information about dataset:')\ndf.info()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-09T15:14:47.239841Z","iopub.execute_input":"2025-08-09T15:14:47.240207Z","iopub.status.idle":"2025-08-09T15:14:47.267544Z","shell.execute_reply.started":"2025-08-09T15:14:47.240183Z","shell.execute_reply":"2025-08-09T15:14:47.266860Z"}},"outputs":[{"name":"stdout","text":"\n Information about dataset:\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 5572 entries, 0 to 5571\nData columns (total 5 columns):\n #   Column      Non-Null Count  Dtype \n---  ------      --------------  ----- \n 0   v1          5572 non-null   object\n 1   v2          5572 non-null   object\n 2   Unnamed: 2  50 non-null     object\n 3   Unnamed: 3  12 non-null     object\n 4   Unnamed: 4  6 non-null      object\ndtypes: object(5)\nmemory usage: 217.8+ KB\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"print('\\Checking for missing values:')\nprint(df.isnull().sum())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-09T15:14:47.268343Z","iopub.execute_input":"2025-08-09T15:14:47.269132Z","iopub.status.idle":"2025-08-09T15:14:47.284391Z","shell.execute_reply.started":"2025-08-09T15:14:47.269114Z","shell.execute_reply":"2025-08-09T15:14:47.283678Z"}},"outputs":[{"name":"stdout","text":"\\Checking for missing values:\nv1               0\nv2               0\nUnnamed: 2    5522\nUnnamed: 3    5560\nUnnamed: 4    5566\ndtype: int64\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"# choose only usefull columns and renamed their\ndf = df[['v1', 'v2']].rename(columns={'v1':'label','v2':'text'})","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-09T15:14:47.285154Z","iopub.execute_input":"2025-08-09T15:14:47.285361Z","iopub.status.idle":"2025-08-09T15:14:47.300455Z","shell.execute_reply.started":"2025-08-09T15:14:47.285347Z","shell.execute_reply":"2025-08-09T15:14:47.299777Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"print('\\nUnique values in column \"Label\":')\nprint(df['label'].unique())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-09T15:14:47.301151Z","iopub.execute_input":"2025-08-09T15:14:47.301341Z","iopub.status.idle":"2025-08-09T15:14:47.316906Z","shell.execute_reply.started":"2025-08-09T15:14:47.301326Z","shell.execute_reply":"2025-08-09T15:14:47.316242Z"}},"outputs":[{"name":"stdout","text":"\nUnique values in column \"Label\":\n['ham' 'spam']\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"print('\\nMissing in the text:', df['text'].isnull().sum())\n\ndisplay(df.sample(5, random_state = 37))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-09T15:14:47.317815Z","iopub.execute_input":"2025-08-09T15:14:47.318375Z","iopub.status.idle":"2025-08-09T15:14:47.336524Z","shell.execute_reply.started":"2025-08-09T15:14:47.318356Z","shell.execute_reply":"2025-08-09T15:14:47.335919Z"}},"outputs":[{"name":"stdout","text":"\nMissing in the text: 0\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"     label                                               text\n895    ham  Superb Thought- \\Be grateful that u dont have ...\n4736   ham  Nt only for driving even for many reasons she ...\n5512   ham                          Just making dinner, you ?\n4553   ham  Try to do something dear. You read something f...\n3415   ham   Uhhhhrmm isnt having tb test bad when youre sick","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>label</th>\n      <th>text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>895</th>\n      <td>ham</td>\n      <td>Superb Thought- \\Be grateful that u dont have ...</td>\n    </tr>\n    <tr>\n      <th>4736</th>\n      <td>ham</td>\n      <td>Nt only for driving even for many reasons she ...</td>\n    </tr>\n    <tr>\n      <th>5512</th>\n      <td>ham</td>\n      <td>Just making dinner, you ?</td>\n    </tr>\n    <tr>\n      <th>4553</th>\n      <td>ham</td>\n      <td>Try to do something dear. You read something f...</td>\n    </tr>\n    <tr>\n      <th>3415</th>\n      <td>ham</td>\n      <td>Uhhhhrmm isnt having tb test bad when youre sick</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":9},{"cell_type":"code","source":"stop_words = set(stopwords.words('english'))\nlemmatizer = WordNetLemmatizer()\n\ndef clean_text(text):\n    # 1. Lowercase\n    text = text.lower()\n    #2. Remove everything that is not letters and spaces\n    text = re.sub(r'[^a-z\\s]','', text)\n    # 3. Break text into words\n    words = text.split()\n    # 4. Remove stop words and lemmatize\n    words = [lemmatizer.lemmatize(word) for word in words if word not in stop_words]\n    #5. Connect back into a string\n    return ' '.join(words)\n\n# check in exampes\nprint(clean_text('Superb Thought- \\\\Be grateful that u dont have ...'))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-09T15:14:47.338111Z","iopub.execute_input":"2025-08-09T15:14:47.338328Z","iopub.status.idle":"2025-08-09T15:14:49.835105Z","shell.execute_reply.started":"2025-08-09T15:14:47.338314Z","shell.execute_reply":"2025-08-09T15:14:49.834455Z"}},"outputs":[{"name":"stdout","text":"superb thought grateful u dont\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"df['clean_text'] = df['text'].apply(clean_text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-09T15:14:49.835857Z","iopub.execute_input":"2025-08-09T15:14:49.836147Z","iopub.status.idle":"2025-08-09T15:14:50.032507Z","shell.execute_reply.started":"2025-08-09T15:14:49.836125Z","shell.execute_reply":"2025-08-09T15:14:50.031950Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"# Splitting into training and validation sets\nX_train, X_val, y_train, y_val = train_test_split(df['clean_text'], df['label'], test_size=0.2, random_state=37, stratify = df['label'])\n\nprint(f'Size of train set:{len(X_train)}')\nprint(f'Size of validation set:{len(X_val)}')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-09T15:14:50.033164Z","iopub.execute_input":"2025-08-09T15:14:50.033394Z","iopub.status.idle":"2025-08-09T15:14:50.052422Z","shell.execute_reply.started":"2025-08-09T15:14:50.033373Z","shell.execute_reply":"2025-08-09T15:14:50.051795Z"}},"outputs":[{"name":"stdout","text":"Size of train set:4457\nSize of validation set:1115\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"# Text vectorization (BoW and TF-IDF)\n# Initialize vectorizers\ncount_vectorizer = CountVectorizer(max_features=5000, ngram_range=(1,2))\ntfidf_vectorizer = TfidfVectorizer(max_features=5000, ngram_range=(1,2))\n\n# We train and transform training data\nX_train_count = count_vectorizer.fit_transform(X_train)\nX_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n\n# Transform validation data\nX_val_count = count_vectorizer.transform(X_val)\nX_val_tfidf = tfidf_vectorizer.transform(X_val)\n\nprint(f'Dimensionality of the BoW matrix: {X_train_count.shape}')\nprint(f'Dimensionality of TF-IDF matrix:{X_train_tfidf.shape}')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-09T15:14:50.053119Z","iopub.execute_input":"2025-08-09T15:14:50.053357Z","iopub.status.idle":"2025-08-09T15:14:50.314481Z","shell.execute_reply.started":"2025-08-09T15:14:50.053332Z","shell.execute_reply":"2025-08-09T15:14:50.313879Z"}},"outputs":[{"name":"stdout","text":"Dimensionality of the BoW matrix: (4457, 5000)\nDimensionality of TF-IDF matrix:(4457, 5000)\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"#Training Logistic Regression models on BoW and TF-IDF\n# Initialize models\nlr_count = LogisticRegression(max_iter = 1000, random_state = 37)\nlr_tfidf = LogisticRegression(max_iter = 1000, random_state = 37)\n\n# Training on BoW\nlr_count.fit(X_train_count, y_train)\n\n# Training on TF-IDF\nlr_tfidf.fit(X_train_tfidf, y_train)\n\n# Predictions\n\ny_pred_count = lr_count.predict(X_val_count)\ny_pred_tfidf = lr_tfidf.predict(X_val_tfidf)\n\n# Spam class probability predictions\ny_proba_count = lr_count.predict_proba(X_val_count)[:, 1]\ny_proba_tfidf = lr_tfidf.predict_proba(X_val_tfidf)[:, 1]\n\n# Rating of model\n\nprint(\"=== Logistic Regression on BoW ===\")\nprint(f\"Accuracy: {accuracy_score(y_val, y_pred_count):.4f}\")\nprint(f\"AUC: {roc_auc_score(y_val.map({'ham':0, 'spam':1}), y_proba_count):.4f}\")\nprint(classification_report(y_val, y_pred_count))\n\nprint(\"\\n=== Logistic Regression on TF-IDF ===\")\nprint(f\"Accuracy: {accuracy_score(y_val, y_pred_tfidf):.4f}\")\nprint(f\"AUC: {roc_auc_score(y_val.map({'ham':0, 'spam':1}), y_proba_tfidf):.4f}\")\nprint(classification_report(y_val, y_pred_tfidf))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-09T15:14:50.314954Z","iopub.execute_input":"2025-08-09T15:14:50.315177Z","iopub.status.idle":"2025-08-09T15:14:50.733433Z","shell.execute_reply.started":"2025-08-09T15:14:50.315161Z","shell.execute_reply":"2025-08-09T15:14:50.732420Z"}},"outputs":[{"name":"stdout","text":"=== Logistic Regression –Ω–∞ BoW ===\nAccuracy: 0.9803\nAUC: 0.9880\n              precision    recall  f1-score   support\n\n         ham       0.98      1.00      0.99       966\n        spam       1.00      0.85      0.92       149\n\n    accuracy                           0.98      1115\n   macro avg       0.99      0.93      0.95      1115\nweighted avg       0.98      0.98      0.98      1115\n\n\n=== Logistic Regression –Ω–∞ TF-IDF ===\nAccuracy: 0.9605\nAUC: 0.9883\n              precision    recall  f1-score   support\n\n         ham       0.96      0.99      0.98       966\n        spam       0.96      0.74      0.83       149\n\n    accuracy                           0.96      1115\n   macro avg       0.96      0.87      0.91      1115\nweighted avg       0.96      0.96      0.96      1115\n\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"# Using pre-trained embeddings (Word2Vec)\n# Download model Word2Vec\nw2v_model = api.load(\"glove-wiki-gigaword-50\")\nprint(\"GloVe model downloaded\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-09T15:49:01.947103Z","iopub.execute_input":"2025-08-09T15:49:01.947353Z","iopub.status.idle":"2025-08-09T15:49:15.600374Z","shell.execute_reply.started":"2025-08-09T15:49:01.947337Z","shell.execute_reply":"2025-08-09T15:49:15.599665Z"}},"outputs":[{"name":"stdout","text":"GloVe model downloaded\n","output_type":"stream"}],"execution_count":40},{"cell_type":"code","source":"# Function to get embedding vectors for text\ndef get_embedding_vector(text, model, vector_size=50):\n    words = text.split()\n    valid_words = [word for word in words if word in model.key_to_index]\n    if not valid_words:\n        return np.zeros(vector_size)\n    word_vectors = [model[word] for word in valid_words]\n    # Make sure that all vectors are the same size\n    for vec in word_vectors:\n        assert len(vec) == vector_size\n    return np.mean(word_vectors, axis=0)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-09T15:23:54.767793Z","iopub.execute_input":"2025-08-09T15:23:54.768452Z","iopub.status.idle":"2025-08-09T15:23:54.773204Z","shell.execute_reply.started":"2025-08-09T15:23:54.768416Z","shell.execute_reply":"2025-08-09T15:23:54.772626Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"# We obtain embedding vectors for the training and validation sets\nX_train_w2v = np.array([get_embedding_vector(text, w2v_model) for text in X_train])\nX_val_w2v = np.array([get_embedding_vector(text, w2v_model) for text in X_val])","metadata":{"trusted":true},"outputs":[],"execution_count":21},{"cell_type":"code","source":"# Training a model on embedding vectors (Logistic Regression)\nlr_w2v = LogisticRegression(max_iter = 1000, random_state = 37)\nlr_w2v.fit(X_train_w2v, y_train)\n\ny_pred_w2v = lr_w2v.predict(X_val_w2v)\ny_proba_w2v = lr_w2v.predict_proba(X_val_w2v)[:,1]\n\nprint(\"=== Logistic Regression on Word2Vec embedding ===\")\nprint(f\"Accuracy: {accuracy_score(y_val, y_pred_w2v):.4f}\")\nprint(f\"AUC: {roc_auc_score(y_val.map({'ham':0, 'spam':1}), y_proba_w2v):.4f}\")\nprint(classification_report(y_val, y_pred_w2v))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-09T15:48:41.537870Z","iopub.execute_input":"2025-08-09T15:48:41.538145Z","iopub.status.idle":"2025-08-09T15:48:41.785882Z","shell.execute_reply.started":"2025-08-09T15:48:41.538124Z","shell.execute_reply":"2025-08-09T15:48:41.784925Z"}},"outputs":[{"name":"stdout","text":"=== Logistic Regression on Word2Vec embedding ===\nAccuracy: 0.8951\nAUC: 0.9257\n              precision    recall  f1-score   support\n\n         ham       0.93      0.95      0.94       966\n        spam       0.62      0.56      0.59       149\n\n    accuracy                           0.90      1115\n   macro avg       0.78      0.75      0.76      1115\nweighted avg       0.89      0.90      0.89      1115\n\n","output_type":"stream"}],"execution_count":39},{"cell_type":"code","source":"#  Random Forest on BoW\nrf_count = RandomForestClassifier(random_state=42, n_jobs=-1)\nrf_count.fit(X_train_count, y_train)\n\ny_pred_rf_count = rf_count.predict(X_val_count)\ny_proba_rf_count = rf_count.predict_proba(X_val_count)[:, 1]\n\nprint(\"=== Random Forest on BoW ===\")\nprint(f\"Accuracy: {accuracy_score(y_val, y_pred_rf_count):.4f}\")\nprint(f\"AUC: {roc_auc_score(y_val.map({'ham':0, 'spam':1}), y_proba_rf_count):.4f}\")\nprint(classification_report(y_val, y_pred_rf_count))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-09T15:48:27.004695Z","iopub.execute_input":"2025-08-09T15:48:27.005400Z","iopub.status.idle":"2025-08-09T15:48:27.635435Z","shell.execute_reply.started":"2025-08-09T15:48:27.005377Z","shell.execute_reply":"2025-08-09T15:48:27.634657Z"}},"outputs":[{"name":"stdout","text":"=== Random Forest on BoW ===\nAccuracy: 0.9767\nAUC: 0.9859\n              precision    recall  f1-score   support\n\n         ham       0.97      1.00      0.99       966\n        spam       0.99      0.83      0.91       149\n\n    accuracy                           0.98      1115\n   macro avg       0.98      0.92      0.95      1115\nweighted avg       0.98      0.98      0.98      1115\n\n","output_type":"stream"}],"execution_count":37},{"cell_type":"code","source":"# Random Forest on TF-IDF\n\nrf_tfidf = RandomForestClassifier(random_state=42, n_jobs=-1)\nrf_tfidf.fit(X_train_tfidf, y_train)\n\ny_pred_rf_tfidf = rf_tfidf.predict(X_val_tfidf)\ny_proba_rf_tfidf = rf_tfidf.predict_proba(X_val_tfidf)[:, 1]\n\nprint(\"=== Random Forest on TF-IDF ===\")\nprint(f\"Accuracy: {accuracy_score(y_val, y_pred_rf_tfidf):.4f}\")\nprint(f\"AUC: {roc_auc_score(y_val.map({'ham':0, 'spam':1}), y_proba_rf_tfidf):.4f}\")\nprint(classification_report(y_val, y_pred_rf_tfidf))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-09T15:40:10.899503Z","iopub.execute_input":"2025-08-09T15:40:10.900026Z","iopub.status.idle":"2025-08-09T15:40:11.571431Z","shell.execute_reply.started":"2025-08-09T15:40:10.899985Z","shell.execute_reply":"2025-08-09T15:40:11.570830Z"}},"outputs":[{"name":"stdout","text":"=== Random Forest on TF-IDF ===\nAccuracy: 0.9767\nAUC: 0.9893\n              precision    recall  f1-score   support\n\n         ham       0.97      1.00      0.99       966\n        spam       1.00      0.83      0.90       149\n\n    accuracy                           0.98      1115\n   macro avg       0.99      0.91      0.95      1115\nweighted avg       0.98      0.98      0.98      1115\n\n","output_type":"stream"}],"execution_count":32},{"cell_type":"code","source":"# XGBoost on TF-IDF\n\nle = LabelEncoder()\ny_train_enc = le.fit_transform(y_train)  # ham -> 0, spam -> 1\ny_val_enc = le.transform(y_val)\n\n\nxgb_tfidf = xgb.XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42)\nxgb_tfidf.fit(X_train_tfidf, y_train_enc)\n\ny_pred_xgb_tfidf = xgb_tfidf.predict(X_val_tfidf)\ny_proba_xgb_tfidf = xgb_tfidf.predict_proba(X_val_tfidf)[:, 1]\n\nprint(\"=== XGBoost on TF-IDF ===\")\nprint(f\"Accuracy: {accuracy_score(y_val_enc, y_pred_xgb_tfidf):.4f}\")\nprint(f\"AUC: {roc_auc_score(y_val_enc, y_proba_xgb_tfidf):.4f}\")\nprint(classification_report(y_val_enc, y_pred_xgb_tfidf, target_names=le.classes_))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-09T15:42:50.274453Z","iopub.execute_input":"2025-08-09T15:42:50.275163Z","iopub.status.idle":"2025-08-09T15:42:50.851355Z","shell.execute_reply.started":"2025-08-09T15:42:50.275139Z","shell.execute_reply":"2025-08-09T15:42:50.850789Z"}},"outputs":[{"name":"stdout","text":"=== XGBoost on TF-IDF ===\nAccuracy: 0.9713\nAUC: 0.9754\n              precision    recall  f1-score   support\n\n         ham       0.97      1.00      0.98       966\n        spam       0.98      0.81      0.88       149\n\n    accuracy                           0.97      1115\n   macro avg       0.97      0.90      0.93      1115\nweighted avg       0.97      0.97      0.97      1115\n\n","output_type":"stream"}],"execution_count":36},{"cell_type":"code","source":"# SVM on TF-IDF\n\nsvm_tfidf = SVC(kernel='linear', probability=True, random_state=42)\nsvm_tfidf.fit(X_train_tfidf, y_train)\n\ny_pred_svm_tfidf = svm_tfidf.predict(X_val_tfidf)\ny_proba_svm_tfidf = svm_tfidf.predict_proba(X_val_tfidf)[:, 1]\n\nprint(\"=== SVM on TF-IDF ===\")\nprint(f\"Accuracy: {accuracy_score(y_val, y_pred_svm_tfidf):.4f}\")\nprint(f\"AUC: {roc_auc_score(y_val.map({'ham':0, 'spam':1}), y_proba_svm_tfidf):.4f}\")\nprint(classification_report(y_val, y_pred_svm_tfidf))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-09T15:41:47.433287Z","iopub.execute_input":"2025-08-09T15:41:47.433860Z","iopub.status.idle":"2025-08-09T15:41:50.454043Z","shell.execute_reply.started":"2025-08-09T15:41:47.433841Z","shell.execute_reply":"2025-08-09T15:41:50.453342Z"}},"outputs":[{"name":"stdout","text":"=== SVM on TF-IDF ===\nAccuracy: 0.9830\nAUC: 0.9918\n              precision    recall  f1-score   support\n\n         ham       0.98      1.00      0.99       966\n        spam       0.97      0.90      0.93       149\n\n    accuracy                           0.98      1115\n   macro avg       0.98      0.95      0.96      1115\nweighted avg       0.98      0.98      0.98      1115\n\n","output_type":"stream"}],"execution_count":35},{"cell_type":"markdown","source":"–ü–æ—Ä—ñ–≤–Ω—è–Ω–Ω—è –º–æ–¥–µ–ª–µ–π –Ω–∞ TF-IDF:\n\n| –ú–æ–¥–µ–ª—å        | Accuracy | AUC    | Precision (spam) | Recall (spam) | F1 (spam) |\n|---------------|----------|--------|------------------|---------------|-----------|\n| LogisticReg   | 0.9614   | 0.9880 | 0.96             | 0.74          | 0.84      |\n| Random Forest | 0.9767   | 0.9893 | 1.00             | 0.83          | 0.90      |\n| SVM           | 0.9830   | 0.9918 | 0.97             | 0.90          | 0.93      |\n| XGBoost       | 0.9713   | 0.9754 | 0.98             | 0.81          | 0.88      |\n","metadata":{}},{"cell_type":"markdown","source":"**–í–∏—Å–Ω–æ–≤–æ–∫:**\n * SVM ‚Äî –Ω–∞–π–∫—Ä–∞—â–∞ –º–æ–¥–µ–ª—å –ø–æ –±–∞–ª–∞–Ω—Å—É —Ç–æ—á–Ω–æ—Å—Ç—ñ —Ç–∞ –ø–æ–≤–Ω–æ—Ç–∏ (precision —Ç–∞ recall).\n\n * Random Forest —ñ XGBoost —Ç–µ–∂ —Å–∏–ª—å–Ω—ñ, –∞–ª–µ —Ç—Ä–æ—Ö–∏ –ø–æ—Å—Ç—É–ø–∞—é—Ç—å—Å—è SVM.\n\n * Logistic Regression ‚Äî –±–∞–∑–æ–≤–∞ –º–æ–¥–µ–ª—å, —Ç–µ–∂ —Ä–æ–±–∏—Ç—å –≥–∞—Ä–Ω—É —Ä–æ–±–æ—Ç—É, –∞–ª–µ —Å–ª–∞–±—à–∞ –∑–∞ –∞–Ω—Å–∞–º–±–ª–µ–≤—ñ –º–µ—Ç–æ–¥–∏.","metadata":{}},{"cell_type":"code","source":"# Hyperparameter search for SVM (GridSearchCV)\nparam_grid = {\n    'C': [0.1, 1, 10],\n    'kernel': ['linear', 'rbf'],\n    'gamma': ['scale', 'auto']  # for 'rbf' kernel\n}\n\nsvm = SVC(probability=True, random_state=37)\n\ngrid_search = GridSearchCV(svm, param_grid, cv=3, scoring='roc_auc', n_jobs=-1, verbose=2)\ngrid_search.fit(X_train_tfidf, y_train_enc)\n\nprint(\"The Best parameters:\", grid_search.best_params_)\nprint(\"The best AUC:\", grid_search.best_score_)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-09T16:15:44.505623Z","iopub.execute_input":"2025-08-09T16:15:44.506240Z","iopub.status.idle":"2025-08-09T16:16:21.192385Z","shell.execute_reply.started":"2025-08-09T16:15:44.506204Z","shell.execute_reply":"2025-08-09T16:16:21.191749Z"}},"outputs":[{"name":"stdout","text":"Fitting 3 folds for each of 12 candidates, totalling 36 fits\nThe Best parameters: {'C': 10, 'gamma': 'scale', 'kernel': 'rbf'}\nThe best AUC: 0.9861314378808341\n","output_type":"stream"}],"execution_count":42},{"cell_type":"code","source":"# Training a model with the best parameters\nbest_svm = grid_search.best_estimator_\ny_pred_best = best_svm.predict(X_val_tfidf)\ny_proba_best = best_svm.predict_proba(X_val_tfidf)[:, 1]\n\nprint(\"=== The Best SVM on TF-IDF ===\")\nprint(f\"Accuracy: {accuracy_score(y_val_enc, y_pred_best):.4f}\")\nprint(f\"AUC: {roc_auc_score(y_val_enc, y_proba_best):.4f}\")\nprint(classification_report(y_val_enc, y_pred_best, target_names=le.classes_))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-09T16:17:50.653240Z","iopub.execute_input":"2025-08-09T16:17:50.653789Z","iopub.status.idle":"2025-08-09T16:17:51.169846Z","shell.execute_reply.started":"2025-08-09T16:17:50.653768Z","shell.execute_reply":"2025-08-09T16:17:51.169107Z"}},"outputs":[{"name":"stdout","text":"=== The Best SVM on TF-IDF ===\nAccuracy: 0.9803\nAUC: 0.9902\n              precision    recall  f1-score   support\n\n         ham       0.98      1.00      0.99       966\n        spam       0.98      0.87      0.92       149\n\n    accuracy                           0.98      1115\n   macro avg       0.98      0.93      0.96      1115\nweighted avg       0.98      0.98      0.98      1115\n\n","output_type":"stream"}],"execution_count":43},{"cell_type":"code","source":"#Let's add engineering features to the data\ndef add_features(df):\n    df = df.copy()\n    df['text_len'] = df['text'].apply(len)\n    df['word_count'] = df['text'].apply(lambda x: len(x.split()))\n    df['digit_count'] = df['text'].apply(lambda x: sum(c.isdigit() for c in x))\n    df['punctuation_count'] = df['text'].apply(lambda x: sum(c in string.punctuation for c in x))\n    return df\n\n# add engineering features to train and validate datasets\ntrain_df = pd.DataFrame({'text': X_train, 'label': y_train})\nval_df = pd.DataFrame({'text': X_val, 'label': y_val})\n\ntrain_df = add_features(train_df)\nval_df = add_features(val_df)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-09T16:24:01.570025Z","iopub.execute_input":"2025-08-09T16:24:01.570297Z","iopub.status.idle":"2025-08-09T16:24:01.622110Z","shell.execute_reply.started":"2025-08-09T16:24:01.570279Z","shell.execute_reply":"2025-08-09T16:24:01.621153Z"}},"outputs":[],"execution_count":50},{"cell_type":"code","source":"# Let's combine TF-IDF vectors with new features\nX_train_features = hstack([X_train_tfidf, train_df[['text_len', 'word_count', 'digit_count', 'punctuation_count']].values])\nX_val_features = hstack([X_val_tfidf, val_df[['text_len', 'word_count', 'digit_count', 'punctuation_count']].values])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-09T16:24:28.958749Z","iopub.execute_input":"2025-08-09T16:24:28.959083Z","iopub.status.idle":"2025-08-09T16:24:28.966474Z","shell.execute_reply.started":"2025-08-09T16:24:28.959061Z","shell.execute_reply":"2025-08-09T16:24:28.965887Z"}},"outputs":[],"execution_count":52},{"cell_type":"code","source":"#Let's train SVM with new features (with better hyperparameters)\nbest_svm.fit(X_train_features, y_train_enc)\n\ny_pred_new = best_svm.predict(X_val_features)\ny_proba_new = best_svm.predict_proba(X_val_features)[:, 1]\n\nprint(\"=== SVM on TF-IDF + engeneer features ===\")\nprint(f\"Accuracy: {accuracy_score(y_val_enc, y_pred_new):.4f}\")\nprint(f\"AUC: {roc_auc_score(y_val_enc, y_proba_new):.4f}\")\nprint(classification_report(y_val_enc, y_pred_new, target_names=le.classes_))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-09T16:24:24.039921Z","iopub.execute_input":"2025-08-09T16:24:24.040228Z","iopub.status.idle":"2025-08-09T16:24:28.957540Z","shell.execute_reply.started":"2025-08-09T16:24:24.040207Z","shell.execute_reply":"2025-08-09T16:24:28.956808Z"}},"outputs":[{"name":"stdout","text":"=== SVM on TF-IDF + engeneer features ===\nAccuracy: 0.8789\nAUC: 0.8661\n              precision    recall  f1-score   support\n\n         ham       0.90      0.97      0.93       966\n        spam       0.59      0.30      0.40       149\n\n    accuracy                           0.88      1115\n   macro avg       0.75      0.63      0.67      1115\nweighted avg       0.86      0.88      0.86      1115\n\n","output_type":"stream"}],"execution_count":51},{"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler()\n\n# Scaling engineering features\ntrain_features_scaled = scaler.fit_transform(train_df[['text_len', 'word_count', 'digit_count', 'punctuation_count']])\nval_features_scaled = scaler.transform(val_df[['text_len', 'word_count', 'digit_count', 'punctuation_count']])\n\n# Combining TF-IDF and scaled features\nX_train_features_scaled = hstack([X_train_tfidf, train_features_scaled])\nX_val_features_scaled = hstack([X_val_tfidf, val_features_scaled])\n\n# We train the model again\nbest_svm.fit(X_train_features_scaled, y_train_enc)\n\ny_pred_scaled = best_svm.predict(X_val_features_scaled)\ny_proba_scaled = best_svm.predict_proba(X_val_features_scaled)[:, 1]\n\nprint(\"=== SVM on TF-IDF + Scaling engineering features ===\")\nprint(f\"Accuracy: {accuracy_score(y_val_enc, y_pred_scaled):.4f}\")\nprint(f\"AUC: {roc_auc_score(y_val_enc, y_proba_scaled):.4f}\")\nprint(classification_report(y_val_enc, y_pred_scaled, target_names=le.classes_))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-09T16:27:22.225822Z","iopub.execute_input":"2025-08-09T16:27:22.226214Z","iopub.status.idle":"2025-08-09T16:27:27.323155Z","shell.execute_reply.started":"2025-08-09T16:27:22.226181Z","shell.execute_reply":"2025-08-09T16:27:27.322372Z"}},"outputs":[{"name":"stdout","text":"=== SVM on TF-IDF + Scaling engineering features ===\nAccuracy: 0.9830\nAUC: 0.9925\n              precision    recall  f1-score   support\n\n         ham       0.98      1.00      0.99       966\n        spam       1.00      0.87      0.93       149\n\n    accuracy                           0.98      1115\n   macro avg       0.99      0.94      0.96      1115\nweighted avg       0.98      0.98      0.98      1115\n\n","output_type":"stream"}],"execution_count":54},{"cell_type":"markdown","source":"**–í–∏—Å–Ω–æ–≤–æ–∫:**\n<br>–ü—Ä–æ—Å—Ç—ñ —ñ–Ω–∂–µ–Ω–µ—Ä–Ω—ñ –æ–∑–Ω–∞–∫–∏ –º–æ–∂—É—Ç—å –±—É—Ç–∏ –∫–æ—Ä–∏—Å–Ω–∏–º–∏, –∞–ª–µ –¥–ª—è —Ä–æ–±–æ—Ç–∏ –∑ –Ω–∏–º–∏ –≤–∞–∂–ª–∏–≤–æ –ø—Ä–∞–≤–∏–ª—å–Ω–æ –º–∞—Å—à—Ç–∞–±—É–≤–∞—Ç–∏ –¥–∞–Ω—ñ, —â–æ–± –∞–ª–≥–æ—Ä–∏—Ç–º –Ω–µ ¬´–∑–∞–ø–ª—É—Ç–∞–≤—Å—è¬ª —á–µ—Ä–µ–∑ —Ä—ñ–∑–Ω—ñ –¥—ñ–∞–ø–∞–∑–æ–Ω–∏ –∑–Ω–∞—á–µ–Ω—å.","metadata":{}},{"cell_type":"markdown","source":"| –ú–æ–¥–µ–ª—å                      | Accuracy | AUC    | Precision (spam) | Recall (spam) | F1 (spam) |\n|----------------------------|----------|--------|------------------|---------------|-----------|\n| Logistic Regression (BoW)   | 0.9803   | 0.9881 | 1.00             | 0.85          | 0.92      |\n| Logistic Regression (TF-IDF)| 0.9614   | 0.9880 | 0.96             | 0.74          | 0.84      |\n| Random Forest (BoW)         | 0.9767   | 0.9859 | 0.99             | 0.83          | 0.91      |\n| Random Forest (TF-IDF)      | 0.9767   | 0.9893 | 1.00             | 0.83          | 0.90      |\n| SVM (TF-IDF)                | 0.9830   | 0.9918 | 0.97             | 0.90          | 0.93      |\n| SVM (TF-IDF + scaled features)| **0.9830** | **0.9925** | **1.00**       | 0.87          | 0.93      |\n| XGBoost (TF-IDF)            | 0.9713   | 0.9754 | 0.98             | 0.81          | 0.88      |\n| Logistic Regression (Word2Vec)| 0.8951 | 0.9257 | 0.62             | 0.56          | 0.59      |\n","metadata":{}},{"cell_type":"markdown","source":"**–û—Å–Ω–æ–≤–Ω—ñ –≤–∏—Å–Ω–æ–≤–∫–∏:**\n* –ù–∞–π–∫—Ä–∞—â—ñ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∏ –ø—Ä–æ–¥–µ–º–æ–Ω—Å—Ç—Ä—É–≤–∞–ª–∞ SVM-–º–æ–¥–µ–ª—å –∑ TF-IDF –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü—ñ—î—é —Ç–∞ –º–∞—Å—à—Ç–∞–±–æ–≤–∞–Ω–∏–º–∏ —ñ–Ω–∂–µ–Ω–µ—Ä–Ω–∏–º–∏ –æ–∑–Ω–∞–∫–∞–º–∏. –í–æ–Ω–∞ –ø–æ—î–¥–Ω—É—î –≤–∏—Å–æ–∫—É —Ç–æ—á–Ω—ñ—Å—Ç—å —ñ –∑–±–∞–ª–∞–Ω—Å–æ–≤–∞–Ω–∏–π –ø–æ–∫–∞–∑–Ω–∏–∫ recall –¥–ª—è –∫–ª–∞—Å—É —Å–ø–∞–º—É, —â–æ —î –∫—Ä–∏—Ç–∏—á–Ω–æ –≤–∞–∂–ª–∏–≤–∏–º –¥–ª—è –∑–∞–¥–∞—á—ñ –≤–∏—è–≤–ª–µ–Ω–Ω—è –Ω–µ–±–∞–∂–∞–Ω–∏—Ö –ø–æ–≤—ñ–¥–æ–º–ª–µ–Ω—å.\n\n* TF-IDF –ø–æ–∫–∞–∑–∞–≤ –∫—Ä–∞—â—ñ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∏, –Ω—ñ–∂ Bag-of-Words (BoW), –æ—Å–æ–±–ª–∏–≤–æ —É –ø–æ—î–¥–Ω–∞–Ω–Ω—ñ –∑ SVM. –¶–µ –ø–æ–≤‚Äô—è–∑–∞–Ω–æ –∑ —Ç–∏–º, —â–æ TF-IDF –≤—Ä–∞—Ö–æ–≤—É—î –Ω–µ —Ç—ñ–ª—å–∫–∏ —á–∞—Å—Ç–æ—Ç—É, –∞ –π –∑–Ω–∞—á—É—â—ñ—Å—Ç—å —Å–ª—ñ–≤ —É –∫–æ–Ω—Ç–µ–∫—Å—Ç—ñ –≤—Å—å–æ–≥–æ –∫–æ—Ä–ø—É—Å—É.\n\n* –ü–æ–ø–µ—Ä–µ–¥–Ω—å–æ –Ω–∞–≤—á–µ–Ω—ñ –µ–º–±–µ–¥—ñ–Ω–≥–∏ Word2Vec –Ω–µ –¥–∞–ª–∏ –∫—Ä–∞—â–∏—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ñ–≤ –≤ –ø–æ—Ä—ñ–≤–Ω—è–Ω–Ω—ñ –∑ –∫–ª–∞—Å–∏—á–Ω–∏–º–∏ –º–µ—Ç–æ–¥–∞–º–∏ –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü—ñ—ó —Ç–µ–∫—Å—Ç—É. –¶–µ –º–æ–∂–µ –±—É—Ç–∏ –ø–æ–≤‚Äô—è–∑–∞–Ω–æ –∑ –æ–±–º–µ–∂–µ–Ω–Ω—è–º–∏ —Ä–æ–∑–º—ñ—Ä–Ω–æ—Å—Ç—ñ, –∫–æ–Ω—Ç–µ–∫—Å—Ç—É —Ç–∞ —Å–ø–µ—Ü–∏—Ñ—ñ–∫–∏ —Ç–µ–∫—Å—Ç—ñ–≤ —É –Ω–∞–±–æ—Ä—ñ –¥–∞–Ω–∏—Ö.\n\n* –Ü–Ω–∂–µ–Ω–µ—Ä–Ω—ñ –æ–∑–Ω–∞–∫–∏ –±–µ–∑ –º–∞—Å—à—Ç–∞–±—É–≤–∞–Ω–Ω—è —Å—É—Ç—Ç—î–≤–æ –ø–æ–≥—ñ—Ä—à—É–≤–∞–ª–∏ —è–∫—ñ—Å—Ç—å –º–æ–¥–µ–ª—ñ, –∞–ª–µ –ø—Ä–∞–≤–∏–ª—å–Ω–µ –º–∞—Å—à—Ç–∞–±—É–≤–∞–Ω–Ω—è —Ü–∏—Ö –æ–∑–Ω–∞–∫ –¥–æ–∑–≤–æ–ª–∏–ª–æ –ø–æ–∫—Ä–∞—â–∏—Ç–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç —ñ –ø—ñ–¥–≤–∏—â–∏—Ç–∏ AUC.\n\n* Random Forest —Ç–∞ XGBoost —Ç–µ–∂ –ø–æ–∫–∞–∑–∞–ª–∏ —Ö–æ—Ä–æ—à—ñ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∏, –∞–ª–µ –≤—Å–µ –∂ –¥–µ—â–æ –ø–æ—Å—Ç—É–ø–∞—é—Ç—å—Å—è –æ–ø—Ç–∏–º—ñ–∑–æ–≤–∞–Ω–æ–º—É SVM.","metadata":{}},{"cell_type":"markdown","source":"**–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü—ñ—ó:**\n* –î–ª—è –∑–∞–≤–¥–∞–Ω—å –∫–ª–∞—Å–∏—Ñ—ñ–∫–∞—Ü—ñ—ó —Å–ø–∞–º—É –≤–∞—Ä—Ç–æ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–≤–∞—Ç–∏ TF-IDF –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü—ñ—é –∑ –æ–ø—Ç–∏–º—ñ–∑–æ–≤–∞–Ω–æ—é SVM-–º–æ–¥–µ–ª–ª—é.\n\n* –í–∞—Ä—Ç–æ –ø—Ä–æ–¥–æ–≤–∂—É–≤–∞—Ç–∏ –≤–¥–æ—Å–∫–æ–Ω–∞–ª—é–≤–∞—Ç–∏ —ñ–Ω–∂–µ–Ω–µ—Ä–Ω—ñ –æ–∑–Ω–∞–∫–∏, –µ–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—É–≤–∞—Ç–∏ –∑ –±—ñ–ª—å—à –ø—Ä–æ—Å—É–Ω—É—Ç–∏–º–∏ –º–æ–¥–µ–ª—è–º–∏ (–Ω–µ–π—Ä–æ–Ω–Ω—ñ –º–µ—Ä–µ–∂—ñ, —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–∏) —ñ —Ä–æ–±–∏—Ç–∏ –∞–Ω–∞–ª—ñ–∑ –ø–æ–º–∏–ª–æ–∫ –¥–ª—è —Ä–æ–∑—É–º—ñ–Ω–Ω—è —Å–ª–∞–±–∫–∏—Ö –º—ñ—Å—Ü—å —Å–∏—Å—Ç–µ–º–∏.\n\n* –î–ª—è –ø–æ–¥–∞–ª—å—à–æ–≥–æ –ø–æ–∫—Ä–∞—â–µ–Ω–Ω—è –º–æ–∂–ª–∏–≤–µ –≤–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è –∞–Ω—Å–∞–º–±–ª—ñ–≤ –º–æ–¥–µ–ª–µ–π.","metadata":{}}]}